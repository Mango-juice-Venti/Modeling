{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c8115c-c14f-43cf-8662-f5f816dff1f5",
   "metadata": {},
   "source": [
    "## **피처 삭제 및 변경 후 catboost + xgboost + lgbm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427c96a-978b-4898-abd2-08a7973892d0",
   "metadata": {},
   "source": [
    "### **valid weighted sMAPE=44.3632**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82b46e5-b1a7-44ed-90ad-1c23aec6791c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4121527312.py, line 615)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 615\u001b[1;36m\u001b[0m\n\u001b[1;33m    test_files = sorted(glob.glob(TEST_GLOB))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "리조트 식음업장 — CatBoost + XGBoost + LightGBM 3중 앙상블\n",
    "+ 시간누수 방지 타깃인코딩/시계열 피처\n",
    "+ 업장가중 sMAPE(담하·미라시아 가중)\n",
    "+ 전역/세그먼트 가중 블렌딩(희소/연회장 아이템 보수적 혼합)\n",
    "+ 7일 재귀예측 + 업장별 캘리브레이션(합계 ratio clip) + 퍼지매칭 제출\n",
    "+ (옵션) 간단 weight grid로 앙상블 가중치 탐색\n",
    "\n",
    "사용법\n",
    "1) BASE_DIR 경로 수정\n",
    "2) re_train_06.csv, TEST_*processed (1).csv, 곤지암sample_submission.csv 준비\n",
    "3) python resort_triple_ensemble_full.py 실행 → submission_triple.csv 생성\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, glob, re, unicodedata, difflib, warnings\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====================== Config ======================\n",
    "BASE_DIR   = Path(r\"C:\\Users\\LG\\Downloads\")  # ← 네 환경에 맞게 수정\n",
    "TRAIN_FILE = BASE_DIR / \"re_train_06.csv\"\n",
    "TEST_GLOB  = str(BASE_DIR / \"TEST_*processed (1).csv\")   # 예: TEST_00_processed (1).csv ~\n",
    "SAMPLE_SUB = BASE_DIR / \"곤지암sample_submission.csv\"\n",
    "OUT_FILE   = BASE_DIR / \"submission_triple.csv\"\n",
    "\n",
    "RANDOM_STATE     = 42\n",
    "VALID_LAST_DAYS  = 28\n",
    "N_FOLDS          = 4  # rolling origin folds\n",
    "\n",
    "# 업장 가중(평가식 w_s)\n",
    "SHOP_WEIGHTS: Dict[str, float] = {\"담하\": 2.0, \"미라시아\": 2.0}\n",
    "\n",
    "# 학습 가중: 양성 부스트(희소 대응)\n",
    "POS_SAMPLE_BOOST = 4.0\n",
    "\n",
    "# 캘리브레이션 clip\n",
    "CAL_CLIP = (0.85, 1.15)\n",
    "\n",
    "# 희소/정체 기준\n",
    "SPARSE_NZ_THRESHOLD  = 0.10\n",
    "STALE_DAYS_THRESHOLD = 21\n",
    "\n",
    "# 재귀예측 후처리\n",
    "WEEKEND_BUMP = 1.05\n",
    "HOLIDAY_BUMP = 1.10\n",
    "SPIKE_BUMP   = 1.06\n",
    "SPIKE_RATIO  = 1.50\n",
    "\n",
    "# 세그먼트 알파(희소/연회장일 때 앵커 혼합 비율↑)\n",
    "ANCHOR_MIX_FOR_SPARSE = 0.30  # yhat = (1-0.30)*model + 0.30*anchor\n",
    "\n",
    "# ====================== Weighted sMAPE ======================\n",
    "def _smape_official_vector(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    m = denom > 0\n",
    "    if not np.any(m):\n",
    "        return np.nan\n",
    "    return float(np.mean(2.0 * np.abs(y_true[m] - y_pred[m]) / denom[m]) * 100.0)\n",
    "\n",
    "def weighted_smape_by_shop_item(df, shop_weights):\n",
    "    \"\"\"\n",
    "    df: columns=['영업장명','영업장명_메뉴명','y_true','y_pred']\n",
    "    - y_true==0 제외 → 아이템 sMAPE → 업장 평균 → 업장 가중\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return float(\"nan\")\n",
    "    work = df[df[\"y_true\"].astype(float) != 0].copy()\n",
    "    if len(work) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    item_scores = []\n",
    "    for (shop, item), sub in work.groupby([\"영업장명\", \"영업장명_메뉴명\"], sort=False):\n",
    "        s = _smape_official_vector(sub[\"y_true\"].values, sub[\"y_pred\"].values)\n",
    "        if not np.isnan(s):\n",
    "            item_scores.append((shop, s))\n",
    "    if not item_scores:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    shop_df = (pd.DataFrame(item_scores, columns=[\"영업장명\", \"s_item\"])\n",
    "               .groupby(\"영업장명\", as_index=False)[\"s_item\"].mean()\n",
    "               .rename(columns={\"s_item\": \"s_shop\"}))\n",
    "    shop_df[\"w\"] = shop_df[\"영업장명\"].map(lambda s: float(shop_weights.get(s, 1.0)))\n",
    "    wsum = float(shop_df[\"w\"].sum())\n",
    "    if wsum <= 0:\n",
    "        return float(shop_df[\"s_shop\"].mean())\n",
    "    return float(np.sum(shop_df[\"s_shop\"] * shop_df[\"w\"]) / wsum)\n",
    "\n",
    "# ====================== IO & Utils ======================\n",
    "def safe_read_csv(path: Path):\n",
    "    for enc in [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"Failed to read {path}\")\n",
    "\n",
    "def ensure_upjang(df):\n",
    "    if \"영업장명\" not in df.columns:\n",
    "        if \"영업장명_메뉴명\" in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"영업장명\"] = df[\"영업장명_메뉴명\"].astype(str).str.split(\"_\", n=1).str[0]\n",
    "        else:\n",
    "            df[\"영업장명\"] = \"\"\n",
    "    return df\n",
    "\n",
    "# ====================== Feature Engineering ======================\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"영업일자\"] = pd.to_datetime(out[\"영업일자\"], errors=\"coerce\")\n",
    "    out = ensure_upjang(out)\n",
    "\n",
    "    if \"매출수량\" in out.columns:\n",
    "        out[\"매출수량\"] = pd.to_numeric(out[\"매출수량\"], errors=\"coerce\").fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "    out = out.sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "\n",
    "    # 기본 캘린더\n",
    "    out[\"dow\"]   = out[\"영업일자\"].dt.weekday\n",
    "    out[\"week\"]  = out[\"영업일자\"].dt.isocalendar().week.astype(int)\n",
    "    out[\"month\"] = out[\"영업일자\"].dt.month\n",
    "    out[\"year\"]  = out[\"영업일자\"].dt.year\n",
    "    out[\"day\"]   = out[\"영업일자\"].dt.day\n",
    "    out[\"woy\"]   = out[\"영업일자\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # Lags & rollings (누수 방지: shift)\n",
    "    for lag in [1,7,14,28]:\n",
    "        out[f\"lag_{lag}\"] = out.groupby(\"영업장명_메뉴명\")[\"매출수량\"].shift(lag)\n",
    "    for win in [7,14,28]:\n",
    "        grp = out.groupby(\"영업장명_메뉴명\")[\"매출수량\"]\n",
    "        out[f\"roll_mean_{win}\"] = grp.shift(1).rolling(win, min_periods=3).mean()\n",
    "        out[f\"roll_std_{win}\"]  = grp.shift(1).rolling(win, min_periods=3).std()\n",
    "        out[f\"roll_max_{win}\"]  = grp.shift(1).rolling(win, min_periods=3).max()\n",
    "        out[f\"roll_min_{win}\"]  = grp.shift(1).rolling(win, min_periods=3).min()\n",
    "\n",
    "    # 시간-안전 타깃 인코딩(shift→expanding)\n",
    "    out[\"exp_item_dow_mean\"] = (\n",
    "        out.groupby([\"영업장명_메뉴명\",\"dow\"])[\"매출수량\"].apply(lambda s: s.shift(1).expanding(min_periods=3).mean())\n",
    "          .reset_index(level=[0,1], drop=True)\n",
    "    )\n",
    "    out[\"exp_item_mean\"] = (\n",
    "        out.groupby(\"영업장명_메뉴명\")[\"매출수량\"].apply(lambda s: s.shift(1).expanding(min_periods=3).mean())\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    out[\"exp_shop_dow_mean\"] = (\n",
    "        out.groupby([\"영업장명\",\"dow\"])[\"매출수량\"].apply(lambda s: s.shift(1).expanding(min_periods=3).mean())\n",
    "          .reset_index(level=[0,1], drop=True)\n",
    "    )\n",
    "    out[\"item_dow_idx\"] = out[\"exp_item_dow_mean\"] / (out[\"exp_item_mean\"] + 1e-6)\n",
    "    out[\"shop_dow_idx\"] = out[\"exp_shop_dow_mean\"] / (\n",
    "        out.groupby(\"영업장명\")[\"exp_shop_dow_mean\"].transform(lambda s: s.shift(1).expanding(min_periods=3).mean()) + 1e-6\n",
    "    )\n",
    "\n",
    "    # Trend & sparsity\n",
    "    out[\"trend_7_1\"]  = out[\"roll_mean_7\"]  / (out[\"lag_1\"] + 1e-6)\n",
    "    out[\"trend_14_7\"] = out[\"roll_mean_14\"] / (out[\"roll_mean_7\"] + 1e-6)\n",
    "    out[\"delta_1_7\"]  = out[\"lag_1\"] - out[\"roll_mean_7\"]\n",
    "    out[\"nonzero_rate_28\"] = (\n",
    "        out.groupby(\"영업장명_메뉴명\")[\"매출수량\"].transform(lambda s: s.shift(1).rolling(28, min_periods=3).apply(lambda x: (x>0).mean(), raw=True))\n",
    "    )\n",
    "\n",
    "    # 마지막 판매 이후 경과일\n",
    "    def days_since_last_sale(g):\n",
    "        last = None; res = []\n",
    "        for d, y in zip(g[\"영업일자\"], g[\"매출수량\"]):\n",
    "            res.append(365 if last is None else (d - last).days)\n",
    "            if y > 0: last = d\n",
    "        return pd.Series(res, index=g.index)\n",
    "    try:\n",
    "        out[\"days_since_last_sale\"] = (\n",
    "            out.groupby(\"영업장명_메뉴명\", group_keys=False).apply(days_since_last_sale, include_groups=False).astype(float)\n",
    "        )\n",
    "    except TypeError:\n",
    "        out[\"days_since_last_sale\"] = (\n",
    "            out.groupby(\"영업장명_메뉴명\", group_keys=False).apply(days_since_last_sale).astype(float)\n",
    "        )\n",
    "\n",
    "    # Fourier & flags\n",
    "    out[\"dow_sin\"], out[\"dow_cos\"] = np.sin(2*np.pi*out[\"dow\"]/7),  np.cos(2*np.pi*out[\"dow\"]/7)\n",
    "    out[\"month_sin\"], out[\"month_cos\"] = np.sin(2*np.pi*out[\"month\"]/12), np.cos(2*np.pi*out[\"month\"]/12)\n",
    "    out[\"woy_sin\"], out[\"woy_cos\"] = np.sin(2*np.pi*out[\"woy\"]/53), np.cos(2*np.pi*out[\"woy\"]/53)\n",
    "\n",
    "    for c in [\"is_spike\",\"is_drop\",\"is_weekday_price\",\"is_weekend_price\",\"is_holiday\",\"banquet_type\"]:\n",
    "        if c not in out.columns: out[c] = 0\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # ID 인코딩\n",
    "    out[\"item_id\"] = out[\"영업장명_메뉴명\"].astype(\"category\").cat.codes\n",
    "    out[\"업장_id\"]  = out[\"영업장명\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # NA 처리\n",
    "    num_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if \"매출수량\" in num_cols: num_cols.remove(\"매출수량\")\n",
    "    out[num_cols] = out[num_cols].fillna(0.0)\n",
    "    return out\n",
    "\n",
    "def feature_columns(df: pd.DataFrame) -> List[str]:\n",
    "    base = [\n",
    "        \"item_id\",\"업장_id\",\n",
    "        \"dow\",\"week\",\"month\",\"year\",\"day\",\"woy\",\"woy_sin\",\"woy_cos\",\n",
    "        \"lag_1\",\"lag_7\",\"lag_14\",\"lag_28\",\n",
    "        \"roll_mean_7\",\"roll_std_7\",\"roll_max_7\",\"roll_min_7\",\n",
    "        \"roll_mean_14\",\"roll_std_14\",\"roll_max_14\",\"roll_min_14\",\n",
    "        \"roll_mean_28\",\"roll_std_28\",\"roll_max_28\",\"roll_min_28\",\n",
    "        \"exp_item_dow_mean\",\"exp_item_mean\",\"exp_shop_dow_mean\",\n",
    "        \"item_dow_idx\",\"shop_dow_idx\",\n",
    "        \"trend_7_1\",\"trend_14_7\",\"delta_1_7\",\n",
    "        \"nonzero_rate_28\",\"days_since_last_sale\",\n",
    "        \"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "        \"is_spike\",\"is_drop\",\"is_weekday_price\",\"is_weekend_price\",\"is_holiday\",\n",
    "        \"banquet_type\"\n",
    "    ]\n",
    "    return [c for c in base if c in df.columns]\n",
    "\n",
    "# ====================== CV Split ======================\n",
    "def build_rolling_folds(dates: pd.Series, n_folds: int = N_FOLDS, valid_days: int = VALID_LAST_DAYS) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    dates = pd.to_datetime(dates)\n",
    "    dmin, dmax = dates.min(), dates.max()\n",
    "    ends = [dmax - pd.Timedelta(days=valid_days*(n_folds - i - 1)) for i in range(n_folds)]\n",
    "    folds = []\n",
    "    for end in ends:\n",
    "        vstart = end - pd.Timedelta(days=valid_days-1)\n",
    "        folds.append((vstart.normalize(), end.normalize()))\n",
    "    uniq = []\n",
    "    for s,e in folds:\n",
    "        if e<=dmax and s>=dmin and (s,e) not in uniq:\n",
    "            uniq.append((s,e))\n",
    "    return uniq\n",
    "\n",
    "def warmup_valid_mask(feat_df, vstart, item_col=\"영업장명_메뉴명\", date_col=\"영업일자\", need_days=28):\n",
    "    last_hist = (feat_df[feat_df[date_col] < vstart]\n",
    "                 .groupby(item_col)[date_col].max())\n",
    "    cutoff = (last_hist + pd.Timedelta(days=need_days)).rename(\"warm_ok_date\").to_frame()\n",
    "    merged = feat_df.merge(cutoff, left_on=item_col, right_index=True, how=\"left\")\n",
    "    return (merged[date_col] >= merged[\"warm_ok_date\"]).fillna(False).values\n",
    "\n",
    "# ====================== 모델 파라미터 ======================\n",
    "XGB_PARAMS_TW = dict(\n",
    "    objective=\"reg:tweedie\", eval_metric=\"rmse\", tree_method=\"hist\",\n",
    "    eta=0.05, max_depth=7, subsample=0.85, colsample_bytree=0.85,\n",
    "    min_child_weight=8, reg_lambda=2.5, reg_alpha=0.0,\n",
    "    max_delta_step=1.0, seed=RANDOM_STATE, tweedie_variance_power=1.2\n",
    ")\n",
    "LGB_PARAMS_TW = dict(\n",
    "    objective=\"tweedie\", metric=\"rmse\", tweedie_variance_power=1.2,\n",
    "    learning_rate=0.05, num_leaves=127, max_depth=-1,\n",
    "    feature_fraction=0.85, bagging_fraction=0.85, bagging_freq=1,\n",
    "    min_data_in_leaf=60, lambda_l1=0.0, lambda_l2=2.0,\n",
    "    verbosity=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "CAT_PARAMS_LOG = dict(\n",
    "    loss_function=\"RMSE\",  # log1p target로 학습\n",
    "    learning_rate=0.05, depth=8, l2_leaf_reg=3.0,\n",
    "    random_seed=RANDOM_STATE, bootstrap_type=\"Bayesian\", bagging_temperature=1.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ====================== 학습/예측 유틸 ======================\n",
    "def train_xgb_tweedie(X_tr, y_tr, w_tr, X_va):\n",
    "    dtr = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
    "    model = xgb.train(XGB_PARAMS_TW, dtr, num_boost_round=2500, evals=[], verbose_eval=False)\n",
    "    pred = model.predict(xgb.DMatrix(X_va))\n",
    "    return model, np.clip(pred, 0, None)\n",
    "\n",
    "def train_lgb_tweedie(X_tr, y_tr, w_tr, X_va):\n",
    "    ltr = lgb.Dataset(X_tr, label=y_tr, weight=w_tr, free_raw_data=False)\n",
    "    model = lgb.train(LGB_PARAMS_TW, ltr, num_boost_round=2500, valid_sets=[])\n",
    "    pred = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    return model, np.clip(pred, 0, None)\n",
    "\n",
    "def train_cat_log1p(X_tr, y_tr, w_tr, X_va):\n",
    "    m = y_tr > 0\n",
    "    X_pos, y_pos, w_pos = X_tr.loc[m], np.log1p(y_tr[m]), w_tr[m]\n",
    "    cat = CatBoostRegressor(**CAT_PARAMS_LOG)\n",
    "    pool_tr = Pool(X_pos, label=y_pos, weight=w_pos)\n",
    "    cat.fit(pool_tr)\n",
    "    pred = cat.predict(X_va)\n",
    "    pred = np.expm1(np.clip(pred, 0, None))\n",
    "    return cat, pred\n",
    "\n",
    "# ====================== 가중 최적화(간단 grid) ======================\n",
    "def search_best_weights(valid_df: pd.DataFrame, preds: List[np.ndarray], step=0.05):\n",
    "    \"\"\"\n",
    "    preds: [p1, p2, p3] (XGB, LGB, CAT) 순서\n",
    "    w 탐색: w1,w2 in [0,1], w3=1-w1-w2, 단 w3>=0\n",
    "    \"\"\"\n",
    "    p1, p2, p3 = preds\n",
    "    best_w, best_sc = (0.34, 0.33, 0.33), 1e18\n",
    "    for w1 in np.arange(0.0, 1.0+1e-9, step):\n",
    "        for w2 in np.arange(0.0, 1.0-w1+1e-9, step):\n",
    "            w3 = 1.0 - w1 - w2\n",
    "            pv = w1*p1 + w2*p2 + w3*p3\n",
    "            pack = valid_df.copy()\n",
    "            pack[\"y_pred\"] = pv\n",
    "            sc = weighted_smape_by_shop_item(pack, SHOP_WEIGHTS)\n",
    "            if sc < best_sc:\n",
    "                best_sc, best_w = sc, (float(w1), float(w2), float(w3))\n",
    "    return best_w, float(best_sc)\n",
    "\n",
    "# ====================== 캘리브레이션 ======================\n",
    "def ratio_clip(a: np.ndarray, b: np.ndarray, clip=CAL_CLIP) -> float:\n",
    "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "    s_true, s_pred = float(np.sum(a)), float(np.sum(b))\n",
    "    if s_pred <= 1e-6: return 1.0\n",
    "    return float(np.clip(s_true/s_pred, clip[0], clip[1]))\n",
    "\n",
    "class Calibrator:\n",
    "    def __init__(self):\n",
    "        self.global_ratio: float = 1.0\n",
    "        self.shop_ratio: Dict[str, float] = {}\n",
    "    def fit(self, df_valid: pd.DataFrame):\n",
    "        df = df_valid[df_valid[\"y_true\"]>0].copy()\n",
    "        self.global_ratio = ratio_clip(df[\"y_true\"].values, df[\"y_pred\"].values)\n",
    "        self.shop_ratio = df.groupby(\"영업장명\").apply(lambda g: ratio_clip(g[\"y_true\"].values, g[\"y_pred\"].values)).to_dict()\n",
    "        return self\n",
    "    def apply(self, yhat: float, shop: str) -> float:\n",
    "        r = self.shop_ratio.get(str(shop), self.global_ratio)\n",
    "        return float(max(0.0, yhat * r))\n",
    "\n",
    "# ====================== 7일 재귀예측 준비 ======================\n",
    "def build_single_row_features(dt, cur_hist, item_id, upjang_id, dow, banquet_code):\n",
    "    def pull(date):\n",
    "        v = cur_hist.get(date, 0.0)\n",
    "        if isinstance(v, pd.Series): v = float(v.sum())\n",
    "        return float(v)\n",
    "    def window_vals(win): return [pull(dt - timedelta(days=n)) for n in range(1, win+1)]\n",
    "    def lag(n): return pull(dt - timedelta(days=n))\n",
    "\n",
    "    vals7, vals14, vals28 = window_vals(7), window_vals(14), window_vals(28)\n",
    "    def roll_stats(vals):\n",
    "        s = pd.Series(vals)\n",
    "        if s.dropna().size >= 3:\n",
    "            return float(s.mean()), float(s.std(ddof=0)), float(s.max()), float(s.min())\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    rm7, rs7, rmax7, rmin7 = roll_stats(vals7)\n",
    "    rm14, rs14, rmax14, rmin14 = roll_stats(vals14)\n",
    "    rm28, rs28, rmax28, rmin28 = roll_stats(vals28)\n",
    "    nz28 = float(np.mean(np.array(vals28) > 0)) if len(vals28) >= 3 else 0.0\n",
    "\n",
    "    dlast = 365.0\n",
    "    for n, v in enumerate(vals28, start=1):\n",
    "        if v > 0: dlast = float(n); break\n",
    "\n",
    "    woy = int(dt.isocalendar().week)\n",
    "    month = dt.month\n",
    "\n",
    "    lags = {\"lag_1\": lag(1), \"lag_7\": lag(7), \"lag_14\": lag(14), \"lag_28\": lag(28)}\n",
    "\n",
    "    return {\n",
    "        \"item_id\": item_id, \"업장_id\": upjang_id,\n",
    "        \"dow\": dow, \"week\": woy, \"month\": month, \"year\": dt.year, \"day\": dt.day,\n",
    "        \"woy\": woy, \"woy_sin\": np.sin(2*np.pi*woy/53), \"woy_cos\": np.cos(2*np.pi*woy/53),\n",
    "        \"roll_mean_7\": rm7, \"roll_std_7\": rs7, \"roll_max_7\": rmax7, \"roll_min_7\": rmin7,\n",
    "        \"roll_mean_14\": rm14, \"roll_std_14\": rs14, \"roll_max_14\": rmax14, \"roll_min_14\": rmin14,\n",
    "        \"roll_mean_28\": rm28, \"roll_std_28\": rs28, \"roll_max_28\": rmax28, \"roll_min_28\": rmin28,\n",
    "        **lags,\n",
    "        \"exp_item_dow_mean\": rm7,\n",
    "        \"exp_item_mean\": rm28,\n",
    "        \"exp_shop_dow_mean\": rm7,\n",
    "        \"item_dow_idx\": 1.0, \"shop_dow_idx\": 1.0,\n",
    "        \"trend_7_1\": rm7 / (lags[\"lag_1\"] + 1e-6),\n",
    "        \"trend_14_7\": rm14 / (rm7 + 1e-6),\n",
    "        \"delta_1_7\": lags[\"lag_1\"] - rm7,\n",
    "        \"nonzero_rate_28\": nz28,\n",
    "        \"days_since_last_sale\": dlast,\n",
    "        \"dow_sin\": np.sin(2*np.pi*dow/7), \"dow_cos\": np.cos(2*np.pi*dow/7),\n",
    "        \"month_sin\": np.sin(2*np.pi*month/12), \"month_cos\": np.cos(2*np.pi*month/12),\n",
    "        \"is_spike\": 0, \"is_drop\": 0,\n",
    "        \"is_weekday_price\": 1 if dow<5 else 0, \"is_weekend_price\": 1 if dow>=5 else 0,\n",
    "        \"is_holiday\": 0,\n",
    "        \"banquet_type\": banquet_code,\n",
    "    }\n",
    "\n",
    "def apply_postprocess(yhat: float, dt, feat_row: dict) -> float:\n",
    "    if dt.weekday() >= 5: yhat *= WEEKEND_BUMP\n",
    "    if feat_row.get(\"is_holiday\",0)==1: yhat *= HOLIDAY_BUMP\n",
    "    try:\n",
    "        if feat_row[\"lag_1\"] > SPIKE_RATIO * (feat_row[\"roll_mean_7\"] + 1e-6): yhat *= SPIKE_BUMP\n",
    "    except Exception: pass\n",
    "    return float(max(0.0, yhat))\n",
    "\n",
    "# ====================== 퍼지매칭 제출 ======================\n",
    "def parse_tag_day(label: str):\n",
    "    if not isinstance(label, str): label = str(label)\n",
    "    m_tag = re.search(r\"(TEST_\\d{2})\", label, flags=re.IGNORECASE)\n",
    "    m_day = re.findall(r\"(\\d+)\", label)\n",
    "    tag = m_tag.group(1).upper() if m_tag else None\n",
    "    k   = int(m_day[-1]) if m_day else None\n",
    "    return tag, k\n",
    "\n",
    "def norm_name(s: str):\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s).strip().lower()\n",
    "    repl = {\"·\":\" \",\"•\":\" \",\"ㆍ\":\" \",\"‧\":\" \",\"–\":\"-\",\"—\":\"-\",\"’\":\"'\", \"“\":\"\\\"\", \"”\":\"\\\"\",\n",
    "            \"（\":\"(\",\"）\":\")\",\"【\":\"[\",\"】\":\"]\"}\n",
    "    for k,v in repl.items(): s = s.replace(k, v)\n",
    "    s = re.sub(r\"[()\\[\\]{}]\", \" \", s)\n",
    "    s = re.sub(r\"[\\/_\\-]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def build_row_index_map(pred_full: pd.DataFrame):\n",
    "    pred_index_map = {}\n",
    "    if pred_full is not None and not pred_full.empty:\n",
    "        for ridx, row in pred_full.iterrows():\n",
    "            t, k = parse_tag_day(ridx)\n",
    "            if (t is not None) and (k is not None):\n",
    "                pred_index_map[(t, k)] = row\n",
    "    return pred_index_map\n",
    "\n",
    "def build_column_mapping(sample_cols, pred_cols, cutoff=0.90):\n",
    "    pred_norm2orig = {}\n",
    "    for c in pred_cols:\n",
    "        nc = norm_name(c)\n",
    "        if nc not in pred_norm2orig: pred_norm2orig[nc] = c\n",
    "    mapping, exact, fuzzy, unmatched = {}, 0, 0, []\n",
    "    pred_norm_keys = list(pred_norm2orig.keys())\n",
    "    for sc in sample_cols:\n",
    "        ns = norm_name(sc)\n",
    "        if ns in pred_norm2orig:\n",
    "            mapping[sc] = pred_norm2orig[ns]; exact += 1\n",
    "        else:\n",
    "            cand = difflib.get_close_matches(ns, pred_norm_keys, n=1, cutoff=cutoff)\n",
    "            if cand: mapping[sc] = pred_norm2orig[cand[0]]; fuzzy += 1\n",
    "            else: mapping[sc] = None; unmatched.append(sc)\n",
    "    return mapping, exact, fuzzy, unmatched\n",
    "\n",
    "def save_submission(sample: pd.DataFrame, pred_full: pd.DataFrame, out_path: Path):\n",
    "    pred_index_map = build_row_index_map(pred_full)\n",
    "    submission = sample.copy()\n",
    "    idx_labels = submission[\"영업일자\"].tolist()\n",
    "    item_cols  = submission.columns.tolist()[1:]\n",
    "\n",
    "    pred_cols = [] if pred_full is None or pred_full.empty else pred_full.columns.tolist()\n",
    "    col_map, exact, fuzzy, unmatch = build_column_mapping(item_cols, pred_cols, cutoff=0.90)\n",
    "\n",
    "    out_vals, matched_rows = [], 0\n",
    "    for lbl in idx_labels:\n",
    "        t, k = parse_tag_day(lbl)\n",
    "        if (t, k) in pred_index_map:\n",
    "            sr = pred_index_map[(t, k)]\n",
    "            row_vals = []\n",
    "            for sc in item_cols:\n",
    "                pc = col_map.get(sc)\n",
    "                v = float(sr.get(pc, 0.0)) if pc is not None else 0.0\n",
    "                row_vals.append(float(max(0.0, v)))\n",
    "            matched_rows += 1\n",
    "        else:\n",
    "            row_vals = [0.0]*len(item_cols)\n",
    "        out_vals.append(row_vals)\n",
    "\n",
    "    final_df = pd.DataFrame(out_vals, columns=item_cols)\n",
    "    final_df[item_cols] = np.round(final_df[item_cols].values, 0)\n",
    "    final_df.insert(0, \"영업일자\", idx_labels)\n",
    "    final_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total_pred_sum = float(np.nansum(final_df[item_cols].to_numpy()))\n",
    "    print(f\"[DBG] matched_rows={matched_rows}/{len(idx_labels)} | exact={exact} fuzzy={fuzzy} unmatch={len(unmatch)} | nonzero_sum={total_pred_sum:.2f}\")\n",
    "    if unmatch[:5]: print(\"[DBG] sample columns not matched (first 5):\", unmatch[:5])\n",
    "    print(f\"[OK] Saved submission: {out_path}  exists={os.path.exists(out_path)}\")\n",
    "\n",
    "# ====================== 메인 ======================\n",
    "def main():\n",
    "    # 1) 데이터 로드\n",
    "    train  = safe_read_csv(TRAIN_FILE)\n",
    "    sample = safe_read_csv(SAMPLE_SUB)\n",
    "\n",
    "    train[\"영업일자\"] = pd.to_datetime(train[\"영업일자\"], errors=\"coerce\")\n",
    "    train = ensure_upjang(train).sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "\n",
    "    feat_df = build_features(train)\n",
    "    feats   = feature_columns(feat_df)\n",
    "    y_all   = feat_df[\"매출수량\"].astype(float).values\n",
    "\n",
    "    # 2) CV 마지막 폴드 valid (블렌딩/캘리브레이션용)\n",
    "    folds = build_rolling_folds(feat_df[\"영업일자\"], n_folds=N_FOLDS, valid_days=VALID_LAST_DAYS)\n",
    "    print(f\"[CV] folds: {folds}\")\n",
    "    vs, ve = folds[-1]\n",
    "    vmask = (feat_df[\"영업일자\"]>=vs) & (feat_df[\"영업일자\"]<=ve)\n",
    "    vmask = vmask & warmup_valid_mask(feat_df, vs)\n",
    "    tmask = ~vmask\n",
    "\n",
    "    X_tr, y_tr = feat_df.loc[tmask, feats], y_all[tmask]\n",
    "    X_va, y_va = feat_df.loc[vmask, feats], y_all[vmask]\n",
    "    shops_tr   = feat_df.loc[tmask, \"영업장명\"].astype(str).values\n",
    "    shops_va   = feat_df.loc[vmask, \"영업장명\"].astype(str).values\n",
    "    items_va   = feat_df.loc[vmask, \"영업장명_메뉴명\"].astype(str).values\n",
    "\n",
    "    w_tr = pd.Series(shops_tr).map(lambda s: SHOP_WEIGHTS.get(s,1.0)).values * np.where(y_tr>0, POS_SAMPLE_BOOST, 1.0)\n",
    "\n",
    "    # 3) 세 모델 학습 & 검증 예측\n",
    "    m_xgb, p_xgb = train_xgb_tweedie(X_tr, y_tr, w_tr, X_va)\n",
    "    m_lgb, p_lgb = train_lgb_tweedie(X_tr, y_tr, w_tr, X_va)\n",
    "    m_cat, p_cat = train_cat_log1p(X_tr, y_tr, w_tr, X_va)\n",
    "\n",
    "    valid_pack = pd.DataFrame({\"영업장명\": shops_va, \"영업장명_메뉴명\": items_va, \"y_true\": y_va})\n",
    "    (wx, wl, wc), sc_val = search_best_weights(valid_pack, [p_xgb, p_lgb, p_cat], step=0.05)\n",
    "    print(f\"[BLEND] best weights (XGB,LGB,CAT)=({wx:.2f},{wl:.2f},{wc:.2f}) | valid weighted sMAPE={sc_val:.4f}\")\n",
    "\n",
    "    # 4) 캘리브레이션(전역 가중 사용)\n",
    "    y_pred_blend_va = wx*p_xgb + wl*p_lgb + wc*p_cat\n",
    "    calib_pack = valid_pack.copy(); calib_pack[\"y_pred\"] = y_pred_blend_va\n",
    "    calibrator = Calibrator().fit(calib_pack)\n",
    "\n",
    "    # 5) 전체 데이터로 재학습\n",
    "    shops_all = feat_df[\"영업장명\"].astype(str).values\n",
    "    w_all = pd.Series(shops_all).map(lambda s: SHOP_WEIGHTS.get(s,1.0)).values * np.where(y_all>0, POS_SAMPLE_BOOST, 1.0)\n",
    "\n",
    "    m_xgb_full, _ = train_xgb_tweedie(feat_df[feats], y_all, w_all, feat_df[feats].iloc[:1])\n",
    "    m_lgb_full, _ = train_lgb_tweedie(feat_df[feats], y_all, w_all, feat_df[feats].iloc[:1])\n",
    "    m_cat_full, _ = train_cat_log1p(feat_df[feats], y_all, w_all, feat_df[feats].iloc[:1])\n",
    "    \n",
    "\n",
    "    # 6) 테스트 7일 재귀예측\n",
    "    test_files = sorted(glob.glob(TEST_GLOB))\n",
    "    print(f\"[Info] Found {len(test_files)} test files: {test_files[:3]}{' ...' if len(test_files)>3 else ''}\")\n",
    "\n",
    "    def forecast_7days_for_testfile(models, feats, train_df, test_df, test_tag, weights):\n",
    "        model_xgb, model_lgb, model_cat = models\n",
    "        wx, wl, wc = weights\n",
    "\n",
    "        test_df = ensure_upjang(test_df.copy())\n",
    "        test_df[\"영업일자\"] = pd.to_datetime(test_df[\"영업일자\"], errors=\"coerce\")\n",
    "        test_df = test_df.sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "        last_date = test_df[\"영업일자\"].max()\n",
    "        items = sorted(test_df[\"영업장명_메뉴명\"].astype(str).unique().tolist())\n",
    "\n",
    "        cols = [\"영업일자\",\"영업장명_메뉴명\",\"매출수량\",\"banquet_type\",\"영업장명\",\"dow\"]\n",
    "        a = ensure_upjang(train_df.copy()); b = ensure_upjang(test_df.copy())\n",
    "        take = lambda df: df[[c for c in cols if c in df.columns]]\n",
    "        hist_src = pd.concat([take(a), take(b)], ignore_index=True)\n",
    "        hist_src[\"영업일자\"] = pd.to_datetime(hist_src[\"영업일자\"], errors=\"coerce\")\n",
    "\n",
    "        results = {k: {} for k in range(1,8)}\n",
    "\n",
    "        # 카테고리 ID 맵\n",
    "        all_items = pd.Categorical(hist_src[\"영업장명_메뉴명\"].astype(str))\n",
    "        item_to_id = {name:i for i,name in enumerate(all_items.categories)}\n",
    "        all_shops = pd.Categorical(hist_src[\"영업장명\"].astype(str))\n",
    "        shop_to_id = {name:i for i,name in enumerate(all_shops.categories)}\n",
    "\n",
    "        for item in items:\n",
    "            g = hist_src[hist_src[\"영업장명_메뉴명\"].astype(str)==item].copy().sort_values(\"영업일자\")\n",
    "            y_series = (g.groupby(\"영업일자\", as_index=True)[\"매출수량\"].sum().astype(float).clip(lower=0.0))\n",
    "            shop = g[\"영업장명\"].dropna().astype(str).iloc[-1] if \"영업장명\" in g.columns and len(g[\"영업장명\"].dropna()) else (item.split(\"_\",1)[0] if \"_\" in item else \"\")\n",
    "            last_bt = int(g[\"banquet_type\"].dropna().astype(int).iloc[-1]) if \"banquet_type\" in g.columns and g[\"banquet_type\"].notna().any() else -1\n",
    "\n",
    "            item_id = item_to_id.get(item, -1)\n",
    "            shop_id = shop_to_id.get(shop, -1)\n",
    "\n",
    "            cur_hist = y_series.copy()\n",
    "            for k in range(1,8):\n",
    "                dt = last_date + timedelta(days=k)\n",
    "                dow = dt.weekday()\n",
    "                feat_row = build_single_row_features(dt, cur_hist, item_id, shop_id, dow, last_bt)\n",
    "                X = pd.DataFrame([feat_row])[feats].fillna(0.0)\n",
    "\n",
    "                y_xgb = float(np.clip(m_xgb_full.predict(xgb.DMatrix(X)), 0, None))\n",
    "                y_lgb = float(np.clip(m_lgb_full.predict(X, num_iteration=m_lgb_full.best_iteration), 0, None))\n",
    "                y_cat = float(np.expm1(np.clip(m_cat_full.predict(X), 0, None)))\n",
    "\n",
    "                # 기본 앙상블\n",
    "                yhat = wx*y_xgb + wl*y_lgb + wc*y_cat\n",
    "\n",
    "                # 세그먼트(희소/연회장/장기무판매) → 앵커 혼합\n",
    "                nz  = float(feat_row.get(\"nonzero_rate_28\", 0.0))\n",
    "                dsl = float(feat_row.get(\"days_since_last_sale\", 999.0))\n",
    "                is_sparse = (nz < SPARSE_NZ_THRESHOLD) or (dsl > STALE_DAYS_THRESHOLD) or (last_bt not in [-1, 0])\n",
    "                if is_sparse:\n",
    "                    anchor = max(feat_row.get(\"lag_7\", 0.0), feat_row.get(\"roll_mean_7\", 0.0))\n",
    "                    yhat = (1.0-ANCHOR_MIX_FOR_SPARSE)*yhat + ANCHOR_MIX_FOR_SPARSE*anchor\n",
    "\n",
    "                # 업장 캘리브레이션\n",
    "                yhat = calibrator.apply(yhat, shop)\n",
    "                # 후처리\n",
    "                yhat = apply_postprocess(yhat, dt, feat_row)\n",
    "\n",
    "                results[k][item] = yhat\n",
    "                cur_hist.loc[dt] = yhat\n",
    "\n",
    "        out_rows = []\n",
    "        for k in range(1,8):\n",
    "            row = {\"영업일자\": f\"{test_tag}+{k}일\"}; row.update(results[k]); out_rows.append(row)\n",
    "        return pd.DataFrame(out_rows).set_index(\"영업일자\")\n",
    "\n",
    "    all_pred_wide = []\n",
    "    for tf in test_files:\n",
    "        test_df = safe_read_csv(Path(tf))\n",
    "        assert {\"영업일자\",\"영업장명_메뉴명\",\"매출수량\"}.issubset(test_df.columns), f\"필수 컬럼 누락: {tf}\"\n",
    "        tag = Path(tf).stem.split(\"_\")[1]  # 예: TEST_00_processed (1).csv → \"00\"\n",
    "        if not tag.isdigit():\n",
    "            tag = re.findall(r\"(\\d+)\", Path(tf).stem)\n",
    "            tag = tag[0] if tag else \"00\"\n",
    "        test_tag = f\"TEST_{tag.zfill(2)}\"\n",
    "        wide = forecast_7days_for_testfile(\n",
    "            models=(m_xgb_full, m_lgb_full, m_cat_full), feats=feats,\n",
    "            train_df=train, test_df=test_df, test_tag=test_tag,\n",
    "            weights=(wx, wl, wc)\n",
    "        )\n",
    "        all_pred_wide.append(wide)\n",
    "\n",
    "    pred_full = pd.concat(all_pred_wide, axis=0) if all_pred_wide else pd.DataFrame()\n",
    "\n",
    "    # 7) 제출 저장\n",
    "    save_submission(sample, pred_full, OUT_FILE)\n",
    "    print(\"[OK] Done. Output:\", OUT_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ad52f-49dd-4fee-86a5-e31befca9877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
